{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mx_lenet_sagemaker.py\n"
     ]
    }
   ],
   "source": [
    "%%file mx_lenet_sagemaker.py\n",
    "import logging\n",
    "from os import path as op\n",
    "import os\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "data_path = os.getcwd()+ \"/\"\n",
    "batch_size = 50\n",
    "num_cpus = 0\n",
    "num_gpus = 1\n",
    "\n",
    "def prep_data(data_path):\n",
    "    \"\"\"\n",
    "    Convert numpy array to mx Nd-array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: the directory that save data.npz.\n",
    "    \"\"\"\n",
    "    data = np.load(find_file(data_path, 'data.npz'))\n",
    "    x_train = data['x_train']\n",
    "    y_train = data['y_train'][:,:1] ## only take the second column of y_train\n",
    "    x_test = data['x_test']\n",
    "    y_test = data['y_test'][:,:1]\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train -= np.mean(x_train)\n",
    "    x_train /= np.std(x_train)\n",
    "    x_test -= np.mean(x_train)\n",
    "    x_test /= np.std(x_train)\n",
    "\n",
    "    img_rows = 256\n",
    "    img_cols = 256\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols) ## reshape it to (448, ) instead of (448,1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "    y_train = y_train.reshape(y_train.shape[0], )\n",
    "    y_test = y_test.reshape(y_test.shape[0], )\n",
    "    print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    train_iter = mx.io.NDArrayIter(x_train, y_train, batch_size, shuffle=True)\n",
    "    val_iter = mx.io.NDArrayIter(x_test, y_test, batch_size)\n",
    "\n",
    "    return train_iter, val_iter\n",
    "\n",
    "def find_file(root_path, file_name):\n",
    "    \"\"\"\n",
    "    Searching for data.npz at its root director, and return a full path for the file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_path: the root directory for data.npz.\n",
    "    file_name: refers to data.npz\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if file_name in files:\n",
    "            return os.path.join(root, file_name)\n",
    "\n",
    "def mx_lenet():\n",
    "    \"\"\"Building a three layer LeNet sytle Convolutional Neural Net using MXNet.\"\"\"\n",
    "    data = mx.sym.var('data')\n",
    "    data_dp = mx.symbol.Dropout(data, p = 0.2) ## 20% of the input that gets dropped out during training time\n",
    "    # first conv layer\n",
    "    conv1 = mx.sym.Convolution(data=data_dp, kernel=(5, 5), num_filter=20)\n",
    "    tanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\n",
    "    pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    # second conv layer\n",
    "    conv2 = mx.sym.Convolution(data=pool1, kernel=(5, 5), num_filter=50)\n",
    "    tanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "    pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    \n",
    "    # third conv layer\n",
    "    conv3 = mx.sym.Convolution(data=pool1, kernel=(5, 5), num_filter=50)\n",
    "    tanh3 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "    pool3 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    \n",
    "    # first fullc layer\n",
    "    flatten = mx.sym.flatten(data=pool3)\n",
    "    fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "    tanh4 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n",
    "    # second fullc\n",
    "    fc2 = mx.sym.FullyConnected(data=tanh4, num_hidden=2)\n",
    "    # softmax loss\n",
    "    return mx.sym.SoftmaxOutput(data=fc2, name='softmax')\n",
    "\n",
    "\n",
    "def train(num_cpus, num_gpus, **kwargs):\n",
    "    \"\"\"\n",
    "    Train the image classification neural net.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_cpus: If train the model on an aws GPS machine, num_cpus = 0 and num_gpus = 1, vice versa.\n",
    "    num_gpus: apply to the same rule above\n",
    "    \"\"\"\n",
    "    train_iter, val_iter = prep_data(data_path)\n",
    "    lenet = mx_lenet()\n",
    "    lenet_model = mx.mod.Module(\n",
    "        symbol=lenet,\n",
    "        context=get_train_context(num_cpus, num_gpus))\n",
    "    logging.getLogger().setLevel(logging.DEBUG)\n",
    "    lenet_model.fit(train_iter,\n",
    "                    eval_data=val_iter,\n",
    "                    optimizer='sgd',\n",
    "                    optimizer_params={'learning_rate': 0.1},\n",
    "                    eval_metric='acc',\n",
    "                    batch_end_callback=mx.callback.Speedometer(batch_size, 16),\n",
    "                    num_epoch=50)\n",
    "    return lenet_model\n",
    "\n",
    "\n",
    "def get_train_context(num_cpus, num_gpus):\n",
    "    \"\"\"\n",
    "    Define the model training instance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_cpus: If train the model on an aws GPS machine, num_cpus = 0 and num_gpus = 1, vice versa.\n",
    "    num_gpus: apply to the same rule above\n",
    "    \"\"\"\n",
    "    if num_gpus > 0:\n",
    "        return mx.gpu()\n",
    "    return mx.cpu()\n",
    "\n",
    "def get_train_context(num_cpus, num_gpus):\n",
    "    if num_gpus > 0:\n",
    "        print(\"It's {} instance\".format(num_gpus))\n",
    "        return mx.gpu()\n",
    "    print(\"It's {} instance\".format(num_cpus))\n",
    "    return mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-552819999234\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-py2-gpu-2018-01-12-18-34-02-618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-01-12 18:40:57,573 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-01-12 18:40:57,573 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-01-12 18:40:59,284 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 1, 'channels': {u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'current_host': u'algo-1', u'hosts': [u'algo-1']}, 'user_script_name': u'mx_lenet_sagemaker.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'training': u'/opt/ml/input/data/training'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'mx_lenet_sagemaker.py', u'sagemaker_submit_directory': u's3://sagemaker-us-east-1-552819999234/sagemaker-mxnet-py2-gpu-2018-01-12-18-34-02-618/source/sourcedir.tar.gz', u'sagemaker_region': u'us-east-1', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'sagemaker-mxnet-py2-gpu-2018-01-12-18-34-02-618', u'sagemaker_container_log_level': 20}, 'hosts': [u'algo-1'], '_ps_port': 8000, 'user_script_archive': u's3://sagemaker-us-east-1-552819999234/sagemaker-mxnet-py2-gpu-2018-01-12-18-34-02-618/source/sourcedir.tar.gz', 'sagemaker_region': u'us-east-1', 'input_dir': '/opt/ml/input', 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 4, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-552819999234/sagemaker-mxnet-py2-gpu-2018-01-12-18-34-02-618/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-01-12 18:40:59,382 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-01-12 18:40:59,489 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m((1457, 3, 256, 256), (365, 3, 256, 256), (1457,), (365,))\u001b[0m\n",
      "\u001b[31mIt's 1 instance\u001b[0m\n",
      "\u001b[31m[18:41:11] src/operator/././cudnn_algoreg-inl.h:106: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:16,614 INFO - root - Epoch[0] Batch [16]#011Speed: 276.15 samples/sec#011accuracy=0.655294\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:18,952 INFO - root - Epoch[0] Train-accuracy=0.692308\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:18,952 INFO - root - Epoch[0] Time cost=6.694\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:20,644 INFO - root - Epoch[0] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:23,591 INFO - root - Epoch[1] Batch [16]#011Speed: 277.92 samples/sec#011accuracy=0.607059\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:25,932 INFO - root - Epoch[1] Train-accuracy=0.755385\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:25,932 INFO - root - Epoch[1] Time cost=5.287\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:27,626 INFO - root - Epoch[1] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:30,574 INFO - root - Epoch[2] Batch [16]#011Speed: 277.79 samples/sec#011accuracy=0.715294\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:32,925 INFO - root - Epoch[2] Train-accuracy=0.532308\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:32,925 INFO - root - Epoch[2] Time cost=5.299\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:34,608 INFO - root - Epoch[2] Validation-accuracy=0.470000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:37,550 INFO - root - Epoch[3] Batch [16]#011Speed: 278.31 samples/sec#011accuracy=0.549412\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:39,892 INFO - root - Epoch[3] Train-accuracy=0.544615\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:39,892 INFO - root - Epoch[3] Time cost=5.285\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:41,587 INFO - root - Epoch[3] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:44,528 INFO - root - Epoch[4] Batch [16]#011Speed: 278.53 samples/sec#011accuracy=0.609412\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:46,877 INFO - root - Epoch[4] Train-accuracy=0.626154\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:46,877 INFO - root - Epoch[4] Time cost=5.291\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:48,562 INFO - root - Epoch[4] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:51,517 INFO - root - Epoch[5] Batch [16]#011Speed: 277.08 samples/sec#011accuracy=0.596471\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:53,862 INFO - root - Epoch[5] Train-accuracy=0.635385\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:53,862 INFO - root - Epoch[5] Time cost=5.299\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:55,559 INFO - root - Epoch[5] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:41:58,523 INFO - root - Epoch[6] Batch [16]#011Speed: 276.17 samples/sec#011accuracy=0.595294\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:00,871 INFO - root - Epoch[6] Train-accuracy=0.589231\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:00,872 INFO - root - Epoch[6] Time cost=5.313\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:02,579 INFO - root - Epoch[6] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:05,536 INFO - root - Epoch[7] Batch [16]#011Speed: 276.86 samples/sec#011accuracy=0.607059\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:07,890 INFO - root - Epoch[7] Train-accuracy=0.653846\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:07,890 INFO - root - Epoch[7] Time cost=5.311\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:09,587 INFO - root - Epoch[7] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:12,557 INFO - root - Epoch[8] Batch [16]#011Speed: 275.88 samples/sec#011accuracy=0.658824\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:14,917 INFO - root - Epoch[8] Train-accuracy=0.618462\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:14,917 INFO - root - Epoch[8] Time cost=5.330\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:16,610 INFO - root - Epoch[8] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:19,578 INFO - root - Epoch[9] Batch [16]#011Speed: 275.96 samples/sec#011accuracy=0.644706\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:21,937 INFO - root - Epoch[9] Train-accuracy=0.684615\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:21,937 INFO - root - Epoch[9] Time cost=5.327\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:23,641 INFO - root - Epoch[9] Validation-accuracy=0.575000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:26,611 INFO - root - Epoch[10] Batch [16]#011Speed: 275.84 samples/sec#011accuracy=0.709412\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:28,973 INFO - root - Epoch[10] Train-accuracy=0.670769\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:28,973 INFO - root - Epoch[10] Time cost=5.332\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:30,668 INFO - root - Epoch[10] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:33,639 INFO - root - Epoch[11] Batch [16]#011Speed: 275.73 samples/sec#011accuracy=0.756471\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:36,007 INFO - root - Epoch[11] Train-accuracy=0.703077\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:36,008 INFO - root - Epoch[11] Time cost=5.340\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:37,728 INFO - root - Epoch[11] Validation-accuracy=0.725000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:40,706 INFO - root - Epoch[12] Batch [16]#011Speed: 275.04 samples/sec#011accuracy=0.743529\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:43,077 INFO - root - Epoch[12] Train-accuracy=0.696923\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:43,077 INFO - root - Epoch[12] Time cost=5.348\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:44,771 INFO - root - Epoch[12] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:47,752 INFO - root - Epoch[13] Batch [16]#011Speed: 274.59 samples/sec#011accuracy=0.768235\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:50,125 INFO - root - Epoch[13] Train-accuracy=0.740000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:50,125 INFO - root - Epoch[13] Time cost=5.354\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:51,824 INFO - root - Epoch[13] Validation-accuracy=0.555000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:54,811 INFO - root - Epoch[14] Batch [16]#011Speed: 274.29 samples/sec#011accuracy=0.765882\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:57,184 INFO - root - Epoch[14] Train-accuracy=0.732308\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:57,184 INFO - root - Epoch[14] Time cost=5.360\u001b[0m\n",
      "\u001b[31m2018-01-12 18:42:58,876 INFO - root - Epoch[14] Validation-accuracy=0.690000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:01,865 INFO - root - Epoch[15] Batch [16]#011Speed: 274.08 samples/sec#011accuracy=0.797647\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:04,242 INFO - root - Epoch[15] Train-accuracy=0.732308\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:04,242 INFO - root - Epoch[15] Time cost=5.365\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:05,938 INFO - root - Epoch[15] Validation-accuracy=0.662500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:08,930 INFO - root - Epoch[16] Batch [16]#011Speed: 273.72 samples/sec#011accuracy=0.774118\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:11,308 INFO - root - Epoch[16] Train-accuracy=0.736923\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:11,308 INFO - root - Epoch[16] Time cost=5.370\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:13,006 INFO - root - Epoch[16] Validation-accuracy=0.555000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2018-01-12 18:43:16,002 INFO - root - Epoch[17] Batch [16]#011Speed: 273.47 samples/sec#011accuracy=0.774118\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:18,378 INFO - root - Epoch[17] Train-accuracy=0.760000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:18,378 INFO - root - Epoch[17] Time cost=5.372\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:20,078 INFO - root - Epoch[17] Validation-accuracy=0.537500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:23,071 INFO - root - Epoch[18] Batch [16]#011Speed: 273.65 samples/sec#011accuracy=0.787059\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:25,450 INFO - root - Epoch[18] Train-accuracy=0.756923\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:25,451 INFO - root - Epoch[18] Time cost=5.372\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:27,143 INFO - root - Epoch[18] Validation-accuracy=0.742500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:30,138 INFO - root - Epoch[19] Batch [16]#011Speed: 273.51 samples/sec#011accuracy=0.756471\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:32,519 INFO - root - Epoch[19] Train-accuracy=0.770769\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:32,519 INFO - root - Epoch[19] Time cost=5.376\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:34,224 INFO - root - Epoch[19] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:37,226 INFO - root - Epoch[20] Batch [16]#011Speed: 272.96 samples/sec#011accuracy=0.756471\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:39,612 INFO - root - Epoch[20] Train-accuracy=0.752308\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:39,612 INFO - root - Epoch[20] Time cost=5.388\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:41,321 INFO - root - Epoch[20] Validation-accuracy=0.552500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:44,325 INFO - root - Epoch[21] Batch [16]#011Speed: 272.67 samples/sec#011accuracy=0.761176\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:46,714 INFO - root - Epoch[21] Train-accuracy=0.790769\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:46,714 INFO - root - Epoch[21] Time cost=5.393\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:48,421 INFO - root - Epoch[21] Validation-accuracy=0.725000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:51,420 INFO - root - Epoch[22] Batch [16]#011Speed: 273.12 samples/sec#011accuracy=0.798824\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:53,807 INFO - root - Epoch[22] Train-accuracy=0.787692\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:53,807 INFO - root - Epoch[22] Time cost=5.386\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:55,509 INFO - root - Epoch[22] Validation-accuracy=0.637500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:43:58,510 INFO - root - Epoch[23] Batch [16]#011Speed: 272.96 samples/sec#011accuracy=0.789412\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:00,902 INFO - root - Epoch[23] Train-accuracy=0.763077\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:00,902 INFO - root - Epoch[23] Time cost=5.393\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:02,620 INFO - root - Epoch[23] Validation-accuracy=0.572500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:05,628 INFO - root - Epoch[24] Batch [16]#011Speed: 272.37 samples/sec#011accuracy=0.770588\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:08,020 INFO - root - Epoch[24] Train-accuracy=0.798462\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:08,020 INFO - root - Epoch[24] Time cost=5.400\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:09,732 INFO - root - Epoch[24] Validation-accuracy=0.537500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:12,743 INFO - root - Epoch[25] Batch [16]#011Speed: 272.06 samples/sec#011accuracy=0.791765\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:15,135 INFO - root - Epoch[25] Train-accuracy=0.786154\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:15,136 INFO - root - Epoch[25] Time cost=5.403\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:16,849 INFO - root - Epoch[25] Validation-accuracy=0.667500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:19,856 INFO - root - Epoch[26] Batch [16]#011Speed: 272.47 samples/sec#011accuracy=0.747059\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:22,243 INFO - root - Epoch[26] Train-accuracy=0.773846\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:22,243 INFO - root - Epoch[26] Time cost=5.394\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:23,945 INFO - root - Epoch[26] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:26,953 INFO - root - Epoch[27] Batch [16]#011Speed: 272.37 samples/sec#011accuracy=0.801176\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:29,337 INFO - root - Epoch[27] Train-accuracy=0.809231\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:29,337 INFO - root - Epoch[27] Time cost=5.392\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:31,047 INFO - root - Epoch[27] Validation-accuracy=0.695000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:34,051 INFO - root - Epoch[28] Batch [16]#011Speed: 272.77 samples/sec#011accuracy=0.769412\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:36,441 INFO - root - Epoch[28] Train-accuracy=0.781538\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:36,441 INFO - root - Epoch[28] Time cost=5.394\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:38,142 INFO - root - Epoch[28] Validation-accuracy=0.680000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:41,148 INFO - root - Epoch[29] Batch [16]#011Speed: 272.62 samples/sec#011accuracy=0.814118\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:43,540 INFO - root - Epoch[29] Train-accuracy=0.798462\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:43,540 INFO - root - Epoch[29] Time cost=5.398\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:45,249 INFO - root - Epoch[29] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:48,258 INFO - root - Epoch[30] Batch [16]#011Speed: 272.20 samples/sec#011accuracy=0.788235\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:50,650 INFO - root - Epoch[30] Train-accuracy=0.792308\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:50,650 INFO - root - Epoch[30] Time cost=5.402\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:52,358 INFO - root - Epoch[30] Validation-accuracy=0.735000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:55,367 INFO - root - Epoch[31] Batch [16]#011Speed: 272.28 samples/sec#011accuracy=0.777647\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:57,759 INFO - root - Epoch[31] Train-accuracy=0.796923\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:57,759 INFO - root - Epoch[31] Time cost=5.401\u001b[0m\n",
      "\u001b[31m2018-01-12 18:44:59,465 INFO - root - Epoch[31] Validation-accuracy=0.570000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:02,479 INFO - root - Epoch[32] Batch [16]#011Speed: 271.66 samples/sec#011accuracy=0.757647\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:04,871 INFO - root - Epoch[32] Train-accuracy=0.795385\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:04,871 INFO - root - Epoch[32] Time cost=5.406\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:06,582 INFO - root - Epoch[32] Validation-accuracy=0.640000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:09,593 INFO - root - Epoch[33] Batch [16]#011Speed: 272.00 samples/sec#011accuracy=0.790588\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:11,986 INFO - root - Epoch[33] Train-accuracy=0.781538\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:11,987 INFO - root - Epoch[33] Time cost=5.404\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:13,707 INFO - root - Epoch[33] Validation-accuracy=0.730000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:16,715 INFO - root - Epoch[34] Batch [16]#011Speed: 272.34 samples/sec#011accuracy=0.812941\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:19,112 INFO - root - Epoch[34] Train-accuracy=0.784615\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:19,112 INFO - root - Epoch[34] Time cost=5.405\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:20,831 INFO - root - Epoch[34] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:23,847 INFO - root - Epoch[35] Batch [16]#011Speed: 271.62 samples/sec#011accuracy=0.797647\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:26,240 INFO - root - Epoch[35] Train-accuracy=0.780000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:26,240 INFO - root - Epoch[35] Time cost=5.409\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:27,950 INFO - root - Epoch[35] Validation-accuracy=0.750000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:30,958 INFO - root - Epoch[36] Batch [16]#011Speed: 272.35 samples/sec#011accuracy=0.792941\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:33,345 INFO - root - Epoch[36] Train-accuracy=0.786154\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:33,346 INFO - root - Epoch[36] Time cost=5.396\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:35,054 INFO - root - Epoch[36] Validation-accuracy=0.540000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:38,069 INFO - root - Epoch[37] Batch [16]#011Speed: 271.71 samples/sec#011accuracy=0.785882\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:40,466 INFO - root - Epoch[37] Train-accuracy=0.816923\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:40,466 INFO - root - Epoch[37] Time cost=5.412\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:42,172 INFO - root - Epoch[37] Validation-accuracy=0.707500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:45,188 INFO - root - Epoch[38] Batch [16]#011Speed: 271.72 samples/sec#011accuracy=0.777647\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:47,582 INFO - root - Epoch[38] Train-accuracy=0.806154\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:47,582 INFO - root - Epoch[38] Time cost=5.411\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:49,292 INFO - root - Epoch[38] Validation-accuracy=0.732500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:52,307 INFO - root - Epoch[39] Batch [16]#011Speed: 271.81 samples/sec#011accuracy=0.818824\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:54,698 INFO - root - Epoch[39] Train-accuracy=0.744615\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:54,698 INFO - root - Epoch[39] Time cost=5.406\u001b[0m\n",
      "\u001b[31m2018-01-12 18:45:56,410 INFO - root - Epoch[39] Validation-accuracy=0.715000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2018-01-12 18:45:59,426 INFO - root - Epoch[40] Batch [16]#011Speed: 271.68 samples/sec#011accuracy=0.803529\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:01,821 INFO - root - Epoch[40] Train-accuracy=0.758462\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:01,821 INFO - root - Epoch[40] Time cost=5.411\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:03,546 INFO - root - Epoch[40] Validation-accuracy=0.542500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:06,558 INFO - root - Epoch[41] Batch [16]#011Speed: 272.02 samples/sec#011accuracy=0.770588\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:08,952 INFO - root - Epoch[41] Train-accuracy=0.790769\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:08,952 INFO - root - Epoch[41] Time cost=5.406\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:10,665 INFO - root - Epoch[41] Validation-accuracy=0.582500\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:13,682 INFO - root - Epoch[42] Batch [16]#011Speed: 271.59 samples/sec#011accuracy=0.794118\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:16,077 INFO - root - Epoch[42] Train-accuracy=0.769231\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:16,077 INFO - root - Epoch[42] Time cost=5.411\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:17,777 INFO - root - Epoch[42] Validation-accuracy=0.705000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:20,788 INFO - root - Epoch[43] Batch [16]#011Speed: 272.04 samples/sec#011accuracy=0.821176\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:23,185 INFO - root - Epoch[43] Train-accuracy=0.784615\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:23,185 INFO - root - Epoch[43] Time cost=5.408\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:24,895 INFO - root - Epoch[43] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:27,903 INFO - root - Epoch[44] Batch [16]#011Speed: 272.28 samples/sec#011accuracy=0.811765\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:30,295 INFO - root - Epoch[44] Train-accuracy=0.803077\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:30,295 INFO - root - Epoch[44] Time cost=5.400\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:32,002 INFO - root - Epoch[44] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:35,010 INFO - root - Epoch[45] Batch [16]#011Speed: 272.28 samples/sec#011accuracy=0.838824\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:37,402 INFO - root - Epoch[45] Train-accuracy=0.810769\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:37,402 INFO - root - Epoch[45] Time cost=5.400\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:39,113 INFO - root - Epoch[45] Validation-accuracy=0.735000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:42,120 INFO - root - Epoch[46] Batch [16]#011Speed: 272.43 samples/sec#011accuracy=0.828235\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:44,516 INFO - root - Epoch[46] Train-accuracy=0.764615\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:44,517 INFO - root - Epoch[46] Time cost=5.403\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:46,220 INFO - root - Epoch[46] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:49,234 INFO - root - Epoch[47] Batch [16]#011Speed: 271.81 samples/sec#011accuracy=0.737647\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:51,630 INFO - root - Epoch[47] Train-accuracy=0.758462\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:51,630 INFO - root - Epoch[47] Time cost=5.410\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:53,343 INFO - root - Epoch[47] Validation-accuracy=0.530000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:56,357 INFO - root - Epoch[48] Batch [16]#011Speed: 271.82 samples/sec#011accuracy=0.765882\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:58,753 INFO - root - Epoch[48] Train-accuracy=0.804615\u001b[0m\n",
      "\u001b[31m2018-01-12 18:46:58,753 INFO - root - Epoch[48] Time cost=5.411\u001b[0m\n",
      "\u001b[31m2018-01-12 18:47:00,474 INFO - root - Epoch[48] Validation-accuracy=0.755000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:47:03,490 INFO - root - Epoch[49] Batch [16]#011Speed: 271.56 samples/sec#011accuracy=0.830588\u001b[0m\n",
      "\u001b[31m2018-01-12 18:47:05,886 INFO - root - Epoch[49] Train-accuracy=0.780000\u001b[0m\n",
      "\u001b[31m2018-01-12 18:47:05,886 INFO - root - Epoch[49] Time cost=5.413\u001b[0m\n",
      "\u001b[31m2018-01-12 18:47:07,611 INFO - root - Epoch[49] Validation-accuracy=0.530000\u001b[0m\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "mxnet_estimator = MXNet(\"mx_lenet_sagemaker.py\", \n",
    "                        role=get_execution_role(), \n",
    "                        train_instance_type=\"ml.p2.xlarge\", \n",
    "                        train_instance_count=1)\n",
    "mxnet_estimator.fit(\"s3://ds-skynet/MX-Net/data\") ## give your s3 bucket address here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
